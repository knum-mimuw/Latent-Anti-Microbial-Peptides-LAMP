{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# HydrAMP training on Colab\n",
    "\n",
    "This Colab notebook trains the simplified HydrAMP variational autoencoder on the public [`pszmk/LAMP-datasets`](https://huggingface.co/datasets/pszmk/LAMP-datasets) release.  It mirrors the reference implementation from the Szczurek lab while:\n",
    "\n",
    "* expanding the latent space to 128 dimensions,\n",
    "* removing decoder conditioning inputs, and\n",
    "* providing lightweight checkpoint helpers for saving and restoring training progress.\n",
    "\n",
    "> **Tip:** Select *Runtime \u2192 Change runtime type \u2192 GPU* in Colab for practical training times.  CPU execution works for demonstration purposes but is slower."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install"
   },
   "source": [
    "!pip install -q datasets==2.19.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "config"
   },
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_LENGTH = 25\n",
    "PAD_TOKEN = \" \"\n",
    "VOCAB = [PAD_TOKEN] + list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "TOKEN_TO_IDX = {token: idx for idx, token in enumerate(VOCAB)}\n",
    "PAD_IDX = TOKEN_TO_IDX[PAD_TOKEN]\n",
    "DATASET_REPO = \"pszmk/LAMP-datasets\"\n",
    "DATA_SOURCE = \"huggingface\"  # choose 'huggingface' for the full dataset or 'sample' for the tiny demo set\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAMPLE_DATA = {\n",
    "    \"train\": [\n",
    "        {\"sequence\": \"GIGKFLHSAKKFGKAFVGEIMNS\"},\n",
    "        {\"sequence\": \"KWKLFKKIEKVGQNIRDGIIKAGPAVAVVGQAT\"},\n",
    "        {\"sequence\": \"ILPWKWPWWPWRR\"},\n",
    "        {\"sequence\": \"LKLKLLLLKLK\"},\n",
    "        {\"sequence\": \"GLFDIVKKVVGAFGSL\"},\n",
    "        {\"sequence\": \"VNWKKVLGKIIKVVTMTTV\"},\n",
    "        {\"sequence\": \"FFHHIFRGIVHVGKTIHRLVTG\"},\n",
    "        {\"sequence\": \"ILPWRWPWWPWRR\"},\n",
    "        {\"sequence\": \"GLWSKIKNVAAAAGKAALNAVN\"},\n",
    "        {\"sequence\": \"KKLFKKILKYL\"},\n",
    "        {\"sequence\": \"LLKKLLKKLLKK\"},\n",
    "        {\"sequence\": \"GKKKKFLKKAKKFG\"},\n",
    "        {\"sequence\": \"GWKRWWWW\"},\n",
    "        {\"sequence\": \"GRWLRRFLRKIRRFRPPYLPRPRPRPV\"},\n",
    "        {\"sequence\": \"KKVLKKSYKLLK\"},\n",
    "        {\"sequence\": \"KWKLFKKIGAVLKVL\"},\n",
    "        {\"sequence\": \"GLFKVLGKKISGLL\"},\n",
    "        {\"sequence\": \"WFKKWWKFK\"},\n",
    "        {\"sequence\": \"LKKIGKKIERVGQNTR\"},\n",
    "        {\"sequence\": \"LRKKLWKKLLKLL\"},\n",
    "    ],\n",
    "    \"validation\": [\n",
    "        {\"sequence\": \"FFRLLHSLGKIIKG\"},\n",
    "        {\"sequence\": \"GKKLFKKKGGH\"},\n",
    "        {\"sequence\": \"GLKLRFEK\"},\n",
    "        {\"sequence\": \"GIGKFLHSAGKFGKAF\"},\n",
    "        {\"sequence\": \"GLFDIVKKLVGAFGSL\"},\n",
    "    ],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-md"
   },
   "source": [
    "## Load and preprocess the dataset\n\nThe Hugging Face dataset exposes peptide sequences as strings.  Each residue is mapped to an integer index, then sequences are padded or truncated to 25 residues (the HydrAMP target length).  By default the notebook downloads the full dataset from Hugging Face.  Set `DATA_SOURCE = \"sample\"` in the configuration cell above if you only want to run the tiny bundled demo split, and the helper will still fall back automatically when the remote download is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dataset-code"
   },
   "source": [
    "def encode_sequence(sequence: str, max_length: int = MAX_LENGTH) -> torch.Tensor:\n",
    "    sequence = sequence.upper()\n",
    "    tokens = [TOKEN_TO_IDX.get(residue, PAD_IDX) for residue in sequence]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([PAD_IDX] * (max_length - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "class LAMPSequenceDataset(Dataset):\n",
    "    def __init__(self, split: list[Dict[str, str]]):\n",
    "        self.encoded = [encode_sequence(item[\"sequence\"]) for item in split]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.encoded)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        return self.encoded[index]\n",
    "\n",
    "\n",
    "def _sequence_iterable(raw_split: Any) -> list[Dict[str, str]]:\n",
    "    if isinstance(raw_split, list):\n",
    "        if raw_split and isinstance(raw_split[0], dict):\n",
    "            return raw_split\n",
    "        return [{\"sequence\": str(item)} for item in raw_split]\n",
    "    data = list(raw_split)\n",
    "    if data and isinstance(data[0], dict):\n",
    "        if \"sequence\" in data[0]:\n",
    "            return data\n",
    "        return [{\"sequence\": item.get(\"sequence\", str(item))} for item in data]\n",
    "    return [{\"sequence\": str(item)} for item in data]\n",
    "\n",
    "\n",
    "def _load_sample_splits() -> tuple[list[Dict[str, str]], list[Dict[str, str]]]:\n",
    "    print(\"Using bundled sample data (demo only).\")\n",
    "    return SAMPLE_DATA[\"train\"], SAMPLE_DATA[\"validation\"]\n",
    "\n",
    "\n",
    "VALID_DATA_SOURCES = {\"huggingface\", \"sample\"}\n",
    "\n",
    "\n",
    "def load_lamp_data(data_source: str = DATA_SOURCE) -> tuple[LAMPSequenceDataset, LAMPSequenceDataset]:\n",
    "    source = data_source.lower()\n",
    "    if source not in VALID_DATA_SOURCES:\n",
    "        raise ValueError(f\"Unknown data source {data_source!r}. Choose 'huggingface' or 'sample'.\")\n",
    "\n",
    "    if source == \"sample\":\n",
    "        train_split, val_split = _load_sample_splits()\n",
    "    else:\n",
    "        try:\n",
    "            train_split = load_dataset(DATASET_REPO, split=\"train\")\n",
    "            try:\n",
    "                val_split = load_dataset(DATASET_REPO, split=\"validation\")\n",
    "            except ValueError:\n",
    "                val_split = load_dataset(DATASET_REPO, split=\"test\")\n",
    "        except Exception as exc:  # noqa: BLE001\n",
    "            print(f\"Falling back to bundled sample data because dataset download failed: {exc}\")\n",
    "            train_split, val_split = _load_sample_splits()\n",
    "\n",
    "    train_seq = _sequence_iterable(train_split)\n",
    "    val_seq = _sequence_iterable(val_split)\n",
    "    return LAMPSequenceDataset(train_seq), LAMPSequenceDataset(val_seq)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = load_lamp_data(DATA_SOURCE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models-md"
   },
   "source": [
    "## Define the HydrAMP encoder, decoder, and checkpoint helpers\n",
    "\n",
    "The custom GRU layers follow the structure from the original HydrAMP project.  The decoder projects the 128-dimensional latent vector into the recurrent state size before unrolling the decoder GRU.  Lightweight `save_checkpoint` and `load_checkpoint` functions store the model, optimiser, and bookkeeping state."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "models-code"
   },
   "source": [
    "class HydrAMPGRU(nn.Module):\n",
    "    def __init__(self, units: int = 66, input_units: int = 66, output_len: int = 25, device: str | torch.device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.output_len = output_len\n",
    "        self.units = units\n",
    "        self.input_units = input_units\n",
    "        self.device = torch.device(device)\n",
    "        self.kernel = nn.Parameter(torch.zeros(size=(input_units, units * 3), device=self.device))\n",
    "        self.recurrent_kernel = nn.Parameter(torch.zeros(size=(units, units * 3), device=self.device))\n",
    "        self.bias = nn.Parameter(torch.zeros(size=(units * 3,), device=self.device))\n",
    "\n",
    "    def cell_forward(self, inputs: torch.Tensor, state: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        h_tm1 = state\n",
    "        matrix_x = torch.matmul(inputs, self.kernel)\n",
    "        matrix_x = matrix_x + self.bias\n",
    "        x_z, x_r, x_h = torch.split(matrix_x, self.units, dim=-1)\n",
    "\n",
    "        matrix_inner = torch.matmul(h_tm1, self.recurrent_kernel[: self.units * 2])\n",
    "        recurrent_z, recurrent_r, recurrent_h = torch.split(matrix_inner, self.units, dim=-1)\n",
    "\n",
    "        z = torch.sigmoid(x_z + recurrent_z)\n",
    "        r = torch.sigmoid(x_r + recurrent_r)\n",
    "\n",
    "        recurrent_h = torch.matmul(r * h_tm1, self.recurrent_kernel[:, 2 * self.units :])\n",
    "        hh = torch.tanh(x_h + recurrent_h)\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        new_state = h\n",
    "        return h, new_state\n",
    "\n",
    "    def forward(self, input_: Optional[torch.Tensor], state: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if input_ is None:\n",
    "            if state is None:\n",
    "                raise ValueError(\"Either input_ or state must be provided to HydrAMPGRU.forward.\")\n",
    "            input_ = torch.zeros((state.shape[0], self.input_units), device=self.device)\n",
    "        if state is None:\n",
    "            state = torch.zeros((input_.shape[0], self.units), device=self.device)\n",
    "        current_output = input_\n",
    "        current_state = state\n",
    "        outputs = []\n",
    "        for _ in range(self.output_len):\n",
    "            current_output, current_state = self.cell_forward(current_output, current_state)\n",
    "            outputs.append(current_output)\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "    def forward_on_sequence(self, input_: torch.Tensor, state: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if state is None:\n",
    "            state = torch.zeros((input_.shape[0], self.units), device=self.device)\n",
    "        current_state = state\n",
    "        outputs = []\n",
    "        for i in range(input_.shape[1]):\n",
    "            current_output, current_state = self.cell_forward(input_[:, i], current_state)\n",
    "            outputs.append(current_output)\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "class HydrAMPDecoder(nn.Module):\n",
    "    def __init__(self, device: str | torch.device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.gru = HydrAMPGRU(units=66, input_units=66, device=self.device)\n",
    "        self.latent_to_state = nn.Linear(128, self.gru.units).to(self.device)\n",
    "        self.lstm = nn.LSTM(66, 100, batch_first=True).to(self.device)\n",
    "        self.dense = nn.Linear(100, len(VOCAB)).to(self.device)\n",
    "\n",
    "    def forward(self, latent_state: torch.Tensor, return_logits: bool = True, gumbel_temperature: float = 0.001) -> torch.Tensor:\n",
    "        latent_state = latent_state.to(self.device)\n",
    "        initial_state = self.latent_to_state(latent_state)\n",
    "        gru_output = self.gru(None, initial_state)\n",
    "        lstm_output = self.lstm(gru_output)[0]\n",
    "        dense_output = self.dense(lstm_output)\n",
    "        if return_logits:\n",
    "            return dense_output\n",
    "        return torch.nn.functional.gumbel_softmax(dense_output, tau=gumbel_temperature, hard=False)\n",
    "\n",
    "\n",
    "class HydrAMPEncoder(nn.Module):\n",
    "    def __init__(self, device: str | torch.device = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(VOCAB), embedding_dim=100, device=self.device)\n",
    "        self.gru1_f = HydrAMPGRU(input_units=100, units=128, device=self.device)\n",
    "        self.gru1_r = HydrAMPGRU(input_units=100, units=128, device=self.device)\n",
    "        self.gru2_f = HydrAMPGRU(input_units=256, units=128, device=self.device)\n",
    "        self.gru2_r = HydrAMPGRU(input_units=256, units=128, device=self.device)\n",
    "        self.mean_linear = nn.Linear(256, 128, device=self.device)\n",
    "        self.logvar_linear = nn.Linear(256, 128, device=self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = x.to(self.device)\n",
    "        embeddings = self.embedding(x)\n",
    "        gru1_f_output = self.gru1_f.forward_on_sequence(embeddings)\n",
    "        gru1_r_output = self.gru1_r.forward_on_sequence(torch.flip(embeddings, (1,)))\n",
    "        gru_1_output = torch.concat([gru1_f_output, torch.flip(gru1_r_output, (1,))], dim=-1)\n",
    "        gru2_f_output = self.gru2_f.forward_on_sequence(gru_1_output)\n",
    "        gru2_r_output = self.gru2_r.forward_on_sequence(torch.flip(gru_1_output, (1,)))\n",
    "        gru_2_output = torch.concat([gru2_f_output[:, -1], gru2_r_output[:, -1]], dim=-1)\n",
    "        mean = self.mean_linear(gru_2_output)\n",
    "        logvar = self.logvar_linear(gru_2_output)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HydrAMPCheckpoint:\n",
    "    encoder_state_dict: Dict[str, Any]\n",
    "    decoder_state_dict: Dict[str, Any]\n",
    "    optimizer_state_dict: Dict[str, Any]\n",
    "    epoch: int\n",
    "    global_step: int\n",
    "    extra: Dict[str, Any]\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"encoder_state_dict\": self.encoder_state_dict,\n",
    "            \"decoder_state_dict\": self.decoder_state_dict,\n",
    "            \"optimizer_state_dict\": self.optimizer_state_dict,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"global_step\": self.global_step,\n",
    "            \"extra\": self.extra,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, Any]) -> \"HydrAMPCheckpoint\":\n",
    "        return cls(\n",
    "            encoder_state_dict=data[\"encoder_state_dict\"],\n",
    "            decoder_state_dict=data[\"decoder_state_dict\"],\n",
    "            optimizer_state_dict=data[\"optimizer_state_dict\"],\n",
    "            epoch=data.get(\"epoch\", 0),\n",
    "            global_step=data.get(\"global_step\", 0),\n",
    "            extra=data.get(\"extra\", {}),\n",
    "        )\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str | Path, encoder: HydrAMPEncoder, decoder: HydrAMPDecoder, optimizer: torch.optim.Optimizer, epoch: int, global_step: int, extra: Optional[Dict[str, Any]] = None) -> None:\n",
    "    checkpoint = HydrAMPCheckpoint(\n",
    "        encoder_state_dict=encoder.state_dict(),\n",
    "        decoder_state_dict=decoder.state_dict(),\n",
    "        optimizer_state_dict=optimizer.state_dict(),\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        extra=extra or {},\n",
    "    )\n",
    "    torch.save(checkpoint.to_dict(), Path(path))\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str | Path, encoder: HydrAMPEncoder, decoder: HydrAMPDecoder, optimizer: Optional[torch.optim.Optimizer] = None, map_location: str | torch.device | None = None) -> HydrAMPCheckpoint:\n",
    "    raw_checkpoint = torch.load(Path(path), map_location=map_location)\n",
    "    checkpoint = HydrAMPCheckpoint.from_dict(raw_checkpoint)\n",
    "    encoder.load_state_dict(checkpoint.encoder_state_dict)\n",
    "    decoder.load_state_dict(checkpoint.decoder_state_dict)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint.optimizer_state_dict)\n",
    "    return checkpoint"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-md"
   },
   "source": [
    "## Instantiate the models and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "init-code"
   },
   "source": [
    "encoder = HydrAMPEncoder(device=DEVICE).to(DEVICE)\n",
    "decoder = HydrAMPDecoder(device=DEVICE).to(DEVICE)\n",
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "global_step = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-md"
   },
   "source": [
    "## Train for a few epochs\n",
    "\n",
    "The loop minimises the evidence lower bound (ELBO) consisting of a categorical cross-entropy reconstruction term plus a KL divergence regulariser that keeps the latent distribution close to a unit Gaussian.  Each epoch saves a checkpoint to the `checkpoints/` directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train-code"
   },
   "source": [
    "def kl_divergence(mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    return -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=1)\n",
    "\n",
    "\n",
    "def run_epoch(epoch: int, train: bool = True) -> float:\n",
    "    global global_step\n",
    "    data_loader = train_loader if train else val_loader\n",
    "    encoder.train(train)\n",
    "    decoder.train(train)\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            mean, logvar = encoder(batch)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            latent = mean + eps * std\n",
    "            logits = decoder(latent)\n",
    "            recon_loss = F.cross_entropy(\n",
    "                logits.view(-1, len(VOCAB)),\n",
    "                batch.view(-1),\n",
    "                ignore_index=PAD_IDX,\n",
    "            )\n",
    "            kl_loss = kl_divergence(mean, logvar).mean()\n",
    "            loss = recon_loss + 0.1 * kl_loss\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                global_step += 1\n",
    "        total_loss += loss.detach().item()\n",
    "        total_batches += 1\n",
    "    avg_loss = total_loss / max(1, total_batches)\n",
    "    if train:\n",
    "        print(f\"Epoch {epoch} train loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch} val loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "history = {\"train\": [], \"val\": []}\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = run_epoch(epoch, train=True)\n",
    "    val_loss = run_epoch(epoch, train=False)\n",
    "    history[\"train\"].append(train_loss)\n",
    "    history[\"val\"].append(val_loss)\n",
    "    checkpoint_path = CHECKPOINT_DIR / f\"hydramp_epoch_{epoch:03d}.pt\"\n",
    "    save_checkpoint(\n",
    "        checkpoint_path,\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        extra={\"train_loss\": train_loss, \"val_loss\": val_loss},\n",
    "    )\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restore-md"
   },
   "source": [
    "## Restore from the latest checkpoint\n",
    "\n",
    "Use `load_checkpoint` to resume training or run inference with a trained model.  The snippet below restores the most recent checkpoint saved in the session."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "restore-code"
   },
   "source": [
    "latest_checkpoint = None\n",
    "if CHECKPOINT_DIR.exists():\n",
    "    checkpoints = sorted(CHECKPOINT_DIR.glob(\"hydramp_epoch_*.pt\"))\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "\n",
    "if latest_checkpoint is not None:\n",
    "    print(f\"Loading checkpoint from {latest_checkpoint}\")\n",
    "    state = load_checkpoint(\n",
    "        latest_checkpoint,\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        map_location=DEVICE,\n",
    "    )\n",
    "    print(\n",
    "        f\"Restored epoch={state.epoch}, global_step={state.global_step}, \"\n",
    "        f\"val_loss={state.extra.get('val_loss'):.4f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No checkpoint found yet.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "name": "HydrAMP training",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}