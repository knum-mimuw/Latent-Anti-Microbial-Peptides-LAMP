trainer:
  max_epochs: 100
  accelerator: auto
  devices: [0, 1]
  precision: 16-mixed
  gradient_clip_val: 1.0
  log_every_n_steps: 10
  # IterableDataset (HF streaming) requires val_check_interval to be 1.0 or an int (k batches).
  val_check_interval: 1.0
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  # Optional: KL warmup to reduce posterior collapse (requires the callback below).
  callbacks:
    - class_path: modelling.src.callbacks.loss_weight_warmup.LinearLossWeightWarmup
      init_args:
        loss_name: kl_divergence
        start_weight: 0.0
        end_weight: 0.001
        warmup_steps: 10000
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      project: lamp
      name: grugru_vae
      save_dir: wandb
      offline: false
      log_model: false
