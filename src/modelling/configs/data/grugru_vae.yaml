data:
  class_path: modelling.src.datamodules.seq_dm.SequenceDataModule
  init_args:
    config:
      train_datasets:
        - name: "train"
          cfg:
            path: "pszmk/LAMP-datasets"
            split: "train"
            streaming: true

      val_datasets:
        - name: "val"
          cfg:
            path: "pszmk/LAMP-datasets"
            split: "validation"
            streaming: false

      train_dataloader:
        batch_size: 128
        num_workers: 16
        pin_memory: true
        shuffle: true
        drop_last: false

      val_dataloader:
        batch_size: 128
        num_workers: 16
        pin_memory: true
        shuffle: false
        drop_last: false

      preprocessing:
        # Convert raw amino-acid sequences to integer token IDs expected by embedding-based models.
        sequence_field: sequence
        input_ids_key: input_ids
        # Provide shifted teacher-forcing inputs for the decoder under the key expected by GRUVAE.forward(..., input=...)
        decoder_input_key: input
        target_key: target
        max_length: 256
        # Special tokens (recommended):
        # PAD=0 (ignored in CE), UNK=1, BOS=2, EOS=3, amino acids from 4..
        pad_token_id: 0
        unk_token_id: 1
        bos_token_id: 2
        eos_token_id: 3
        remove_columns: true
        vocab:
          A: 4
          C: 5
          D: 6
          E: 7
          F: 8
          G: 9
          H: 10
          I: 11
          K: 12
          L: 13
          M: 14
          N: 15
          P: 16
          Q: 17
          R: 18
          S: 19
          T: 20
          V: 21
          W: 22
          Y: 23
